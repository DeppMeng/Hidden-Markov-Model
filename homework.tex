\documentclass[runningheads]{llncs}
    \usepackage{graphicx}
    \usepackage{amsmath,amssymb} % define this before the line numbering.
    \usepackage{ruler}
    \usepackage{color}
    \usepackage[width=122mm,left=12mm,paperwidth=146mm,height=193mm,top=12mm,paperheight=217mm]{geometry}
    \usepackage{multirow, multicol}
        
    % \newtheorem{theorem}{Theorem}[section]
    % \newtheorem{lemma}[theorem]{Lemma}
    % \newtheorem{definition}{Definition}[section]
    
    \begin{document}
    % \renewcommand\thelinenumber{\color[rgb]{0.2,0.5,0.8}\normalfont\sffamily\scriptsize\arabic{linenumber}\color[rgb]{0,0,0}}
    % \renewcommand\makeLineNumber {\hss\thelinenumber\ \hspace{6mm} \rlap{\hskip\textwidth\ \hspace{6.5mm}\thelinenumber}}
    % \linenumbers
    \pagestyle{headings}
    \mainmatter
    \def\ECCV18SubNumber{1452}  % Insert your submission number here
    
    \title{Homework} % Replace with your title
    
    \titlerunning{ECCV-18 submission ID \ECCV18SubNumber}
    
    \authorrunning{ECCV-18 submission ID \ECCV18SubNumber}
    
    \author{Depu Meng}
    \institute{Oct. $2018$}
    
    
    \maketitle
    \subsection{}
    \begin{proof}
        (1). From the definition of measure, we have
        \begin{equation}
            \mu (\bigcup_{n=1}^\infty E_n) = \sum_{n=1}^\infty \mu(E_n)
        \end{equation}
        if $E_i \bigcap E_j = \varnothing, i \neq j$.
        So for any $E \in \mathcal{F}$,
        \begin{equation}
            \mu(E) = \mu ( \varnothing \bigcup E) = \mu(E) + \mu(\varnothing)
        \end{equation}
        \par
        that is, $\mu(\varnothing) = 0$.
        \par
        (2).Consider $E_1, E_2,..., E_n, E_{n+1}, ... \in \mathcal{F}$,
        $E_i \bigcap E_j = \varnothing, i \neq j$, then from the definition
        we have
        \begin{align}
            \mu (\bigcup_{k=1}^\infty E_k) &= \sum_{k=1}^\infty \mu(E_k) \\
            \mu (\bigcup_{k=n+1}^\infty E_k) &= \sum_{k=n+1}^\infty \mu(E_k)
        \end{align}
        so that we have
        \begin{align}
            \mu (\bigcup_{k=1}^n E_k) &= \sum_{k=1}^n \mu(E_k)
        \end{align}
        \par
        (3). If $E_1 \subset E_2$, then $E_2 = E_1 \bigcup (E_2 - E_1)$,
        apparently $E_1 \bigcap (E_2 - E_1) = \varnothing$.
        Then from the definition we have
        \begin{align}
            \mu(E_2) = \mu ( E_1 \bigcup (E_2 - E_1)) = \mu(E_1) + \mu(E_2 - E_1) \geq \mu(E_1)
        \end{align}
    \end{proof}
    \subsection{}
    \begin{proof}
        \begin{align}
            P(A) &= P(A \bigcap (\bigcup_{i=1}^\infty B_i)) 
            = P(\bigcup_{i=1}^\infty (A \bigcap B_i)) \\
            &= \sum_{i=1}^\infty P(A \bigcap B_i)
            = \sum_{i=1}^\infty P(B_i) P(A | B_i)
        \end{align}
    \end{proof}
    \subsection{}
    \begin{proof}
        \begin{align}
            \int_0^\infty (1 - F(t))dt &= \int_0^\infty (1 - P\{ X \leq t \})dt \\
            &= \int_0^\infty P \{ X > t \}dt \\
            &= \int_0^\infty \int_t^\infty f(s)dsdt \\
            &= \int_0^\infty \int_0^s f(s) dt ds \\
            &= \int_0^\infty s f(s) ds \\
            &= E[X]
        \end{align}
    \end{proof}
    \subsection{}
    \begin{proof}
        From property (2), we have
        \begin{align}
            P \{ N(s + t) - N(s) = k \} = P \{ N(t) = k \}
        \end{align}
        Firstly consider $k = 0$, denote $P_k(t) = P \{ N(t) = k \}$,
        apparently for $h > 0$
        \begin{align}
            P \{ N(t + h) = 0 \} &= P \{ N(t) = 0, N(t + h) - N(t) = 0 \} \\
            &= P_0(t)P_0(h)
        \end{align}
        On the other hand, from property (3), (4), we have
        \begin{align}
            P_0(h) = 1 - (\lambda h + o(h))
        \end{align}
        So that
        \begin{align}
            \frac{P_0(t + h) - P_0(t)}{h} = -(\lambda P_0(t) + \frac{o(h)}{h})
        \end{align}
        Let $h \rightarrow 0$,
        \begin{align}
            P_0'(t) = - \lambda P_0(t)
        \end{align}
        By solving this differential equation with constraint $P_0(0) = 1$,
        we can get
        \begin{align}
            P_0(t) = e^{-\lambda t}
        \end{align}
        when $n > 0$,
        similarly we have
        \begin{align}
            P_n(t + h) = P_n(t) (1 - \lambda h - o(h)) + 
            P_{n-1}(t) (\lambda h + o(h)) + P_{n-2}(t) o(h)
        \end{align}
        that is
        \begin{align}
            P_n'(t) = -\lambda P_n(t) + \lambda P_{n-1}(t) \\
            \frac{d}{dt}[e^{\lambda t} P_n(t)] = e^{\lambda t} P_{n-1}(t)
        \end{align}
        Notice that $P_0(t) = e^{-\lambda t}$, so we have
        \begin{align}
            P_1(t) = \lambda t e^{-\lambda t}
        \end{align}
        then apparently we have
        \begin{align}
            P_n(t) = \frac{(\lambda t)^n}{n!} e^{-\lambda t}
        \end{align}
    \end{proof}
    \subsection{}
    Denote the transition matrix of $Y$, $Q = [Q_{ij}]$,
    the steady distribution of $Y$ $y$.
    Then we have $Q_{ij} = p_i, i = j$, $Q_{ij} = (1 - p_i)P_{ij}, i \neq j$.
    \par
    For steady distribution $\pi$, we have
    \begin{align}
        \pi P &= \pi \\
        \sum_{i=1}^K \pi_i P_{ij} &= \pi_j, \forall j \in \Phi
    \end{align}
    For steady distribution $y$, we have
    \begin{align}
        y Q &= y \\
        \sum_{i=1}^K y_i Q_{ij} &= y_j, \forall j \in \Phi \\
        \sum_{i=1, i \neq j}^K y_i (1 - p_i)P_{ij}
         &= (1 - p_j) y_j, \forall j \in \Phi 
    \end{align}
    apparently $y' = (y_1(1 - p_1), y_2(1 - p_2)...y_K(1 - p_K)$
    is a solution of equation $(28)$.
    so that we have
    \begin{align}
        y_i = \frac{\pi_i}{(1-p_i)} / \sum_{j = 1}^K \frac{\pi_j}{(1-p_j)}
    \end{align}
    \subsection{}
    \begin{proof}
        (1) From Theory 1.15, we have
        \begin{align}
            P \{ X_{n+1} = j, T_{n+1} - T_n \leq t | X_0, ..., X_n = i; T_0,...,T_n \}
             = P_{ij} (1 - e^{-\lambda(i) t})
        \end{align}
        Let $t \rightarrow \infty$, we have
        \begin{align}
            P \{ X_{n+1} = j | X_0,..., X_n = i \} = P_{ij} = P \{ X_{n+1} = j | X_n = i \}
        \end{align}
        (2) 
        Due to
        \begin{align}
            P_{ij} \leq 0, P_{ii} = 0, \sum_{j \in \Phi} P_{ij} = \sum_{j \neq i, j \in \Phi} P_{ij} = 1
        \end{align}
        we have
        \begin{align}
            P \{ X_{n+1} = j, T_{n+1} - T_n \leq t | X_n = i \}
            &= P_{ij} (1 - e^{-\lambda(i) t}) \\
            \sum_{j=1, j \neq i}^K P \{ X_{n+1} = j, T_{n+1} - T_n \leq t | X_n = i\}
            &= \sum_{j=1, j \neq i}^K P_{ij} (1 - e^{-\lambda(i) t}) \\
            P \{ X_{n+1} \neq i, T_{n+1} - T_n \leq t | X_n = i\}
            &= (1 - e^{-\lambda(i) t})
        \end{align}
        That is,
        \begin{align}
            P \{ X_{n+1} = i, T_{n+1} - T_n \leq t | X_n = i\}
            &= e^{-\lambda(i) t}
        \end{align}
    \end{proof}
    \subsection{}
    \begin{proof}
        Denote $P \{ \alpha_n \} = P \{ X_1 = i_1,..., X_n = i_n, Y_1 = v_{j_1},..., Y_n = v_{j_n} \}$.
        Then we have
        \begin{align}
            P_n &= P \{ X_1 = i_1,..., X_n = i_n, Y_1 = v_{j_1},..., Y_n = v_{j_n} \} \\
            &= P \{X_n = i_n, Y_n = v_{j_n} |\alpha_{n-1} \} P \{\alpha_{n-1} \} \\
            &= P \{Y_n = v_{j_n} | \alpha_{n-1}, X_n = i_n \} P \{X_n = i_n |\alpha_{n-1} \} P \{\alpha_{n-1} \} \\
            &= P \{Y_n = v_{j_n} | X_n = i_n \} P \{X_n = i_n | X_{n-1} = i_{n-1} \} P \{\alpha_{n-1} \} \\
            &= b_{i_n j_n} a_{i_{n-1} i_n} P \{ \alpha_{n-1} \}
        \end{align}
        Notice that $P \{ \alpha_1 \} = \pi_{i_1} b_{i_1 j_1}$, from
        above, we can get that
        \begin{align}
            P \{ \mathbf{X} = x, \mathbf{Y} = y \} = \pi_{i_1} b_{i_1 j_1} ...
             a_{i_{N-1} i_N} b_{i_N j_N}
        \end{align}
    \end{proof}
    \subsection{}
    \begin{proof}
        \begin{align}
            P \{ \mathbf{Y} = y | \mathbf{X} = x \} &= 
            \frac{P \{ \mathbf{Y} = y, \mathbf{X} = x \}}{P \{ \mathbf{X} = x\}} \\
            &= b_{i_1 j_1}...b_{i_N j_N}
        \end{align}
    \end{proof}
    \subsection{}
    \begin{proof}
        \begin{align}
            &\alpha_{n+1}(i) 
            = P \{ Y_1 = v_{j_1}, Y_n = v_{j_n}, Y_{n+1} = v_{j_{n+1}}, X_{n+1} = i | \lambda \} \\
            &= \sum_{k=1}^K P \{ Y_1 = v_{j_1},..., Y_n = v_{j_n}, Y_{n+1} = v_{j_{n+1}}, X_{n} = k, X_{n+1} = i | \lambda \} \\
            &= \sum_{k=1}^K P_A P_B P_C
        \end{align}
        where
        \begin{align}
            &P_A = P \{ Y_1 = v_{j_1},..., Y_n = v_{j_n}, X_{n} = k | \lambda \} \\
            &P_B = P \{ X_{n+1} = i | Y_1 = v_{j_1},..., Y_n = v_{j_n}, X_{n} = k, \lambda \} \\ 
            &P_C = P \{ Y_{n+1} = v_{j_{n+1}} | Y_1 = v_{j_1},..., Y_n = v_{j_n}, X_{n} = k, X_{n+1} = i, \lambda\}
        \end{align}
        Notice that for $P_A$, we have
        \begin{align}
            P_A = P \{ Y_1 = v_{j_1},..., Y_n = v_{j_n}, X_{n} = k | \lambda \} = \alpha_n(k) \\
        \end{align}
        For $P_B$, we have
        \begin{align}
            P_B &= P \{ X_{n+1} = i | Y_1 = v_{j_1},..., Y_n = v_{j_n}, X_{n} = k, \lambda \} \\
            &= P \{ X_{n+1} = i | X_{n} = k, \lambda \} \\
            &= a_{ki}
        \end{align}
        For $P_C$, we have
        \begin{align}
            P_C &= P \{ Y_{n+1} = v_{j_{n+1}} | Y_1 = v_{j_1},..., Y_n = v_{j_n}, X_{n} = k, X_{n+1} = i, \lambda\} \\
            &= P \{ Y_{n+1} = v_{j_{n+1}} | X_{n+1} = i, \lambda\} \\
            &= b_{ij_{n+1}}
        \end{align}
        Combine the three equations together, we have
        \begin{align}
            \alpha_{n+1}(i) = \sum_{k=1}^K P_A P_B P_C = \sum_{k=1}^K \alpha_n(k) a_{ki} b_{ij_{n+1}}
        \end{align}
    \end{proof}
    \subsection{}
    \begin{proof}
        \begin{align}
            \beta_{n}(i) &= P \{ Y_{n+1} = v_{j_{n+1}},...,Y_N = v_{j_N} | X_n = i, \lambda \} \\
            &= \sum_{k=1}^K P \{ Y_{n+1} = v_{j_{n+1}},...,Y_N = v_{j_N}, X_{n+1} = k | X_n = i, \lambda \} \\
            &= \sum_{k=1}^K P_X P_Y P_Z
        \end{align}
        where
        \begin{align}
            &P_X = P \{ X_{n+1} = k | X_n = i, \lambda \} \\
            &P_Y = P \{ Y_{n+2} = v_{j_{n+1}},..., Y_N = v_{j_n} | X_{n+1} = k, X_n = i, \lambda \} \\
            &P_Z = P \{ Y_{n+1} = v_{j_{n+1}} | X_{n+1} = k, X_n = i, Y_{n+2} = v_{j_{n+1}},..., Y_N = v_{j_n}, \lambda \}
        \end{align}
        For $P_X$, we have
        \begin{align}
            P_X = P \{ X_{n+1} = k | X_n = i, \lambda \} = a_{ik}
        \end{align}
        For $P_Y$, we have
        \begin{align}
            P_Y &= P \{ Y_{n+2} = v_{j_{n+1}},..., Y_N = v_{j_n} | X_{n+1} = k, X_n = i, \lambda \} \\
            &= P \{ Y_{n+2} = v_{j_{n+1}},..., Y_N = v_{j_n} | X_{n+1} = k, \lambda \} \\
            &= \beta_{n+1}(k)
        \end{align}
        For $P_Z$, we have
        \begin{align}
            P_Z &= P \{ Y_{n+1} = v_{j_{n+1}} | X_{n+1} = k, X_n = i, Y_{n+2} = v_{j_{n+1}},..., Y_N = v_{j_n}, \lambda \} \\
            &= P \{ Y_{n+1} = v_{j_{n+1}} | X_{n+1} = k, \lambda \} \\
            &= b_{kj_{n+1}}
        \end{align}
        Combine the three equations, we have
        \begin{align}
            \beta_n(i) = \sum_{k=1}^K P_X P_Y P_Z = \sum_{k=1}^K \beta_{n+1}(k) a_{ik} b_{kj_{n+1}}
        \end{align}
    \end{proof}
    \subsection{}
    \begin{proof}
        \begin{align}
            &\delta_{n+1}(i) \\
            &= \max_{i_1...i_{n}} P \{ X_{n+1} = i,..., X_1 = i_1; Y_{n+1} = v_{j_{n+1}},..., Y_1 = v_{j_1} |\lambda \} \\
            &= b_{i j_{n+1}} \max_{i_1...i_{n}} P \{ X_{n+1} = i,..., X_1 = i_1; Y_{n} = v_{j_n},..., Y_1 = v_{j_1} |\lambda \} \\
            &= b_{i j_{n+1}} \max_{i_n} \max_{i_1...i_{n-1}} [ P \{ X_{n} = i_n,..., X_1 = i_1; Y_{n} = v_{j_n},..., Y_1 = v_{j_1} |\lambda \} \\
            & \ \ \ \ P \{ X_{n+1} = i | X_{n} = i_n,..., X_1 = i_1; Y_{n} = v_{j_n},..., Y_1 = v_{j_1} |\lambda \} ] \\
            &= b_{i j_{n+1}} \max_{i_n} [a_{i_n i} \delta_{n}(i_n)]
        \end{align}
    \end{proof}
    \subsection{}
    \begin{proof}
        \begin{align}
            &P \{ X_n = i, X_{n+1} = j, \mathbf{Y} = y | \lambda \} \\
            &= P \{ X_n = i, Y_1 = v_{j_1},..., Y_n = v_{j_n} | \lambda \} \\
            & \ \ \ \ P \{ X_{n+1} = j, Y_{n+1} = v_{j_{n+1}},..., Y_N = v_{j_N} | X_n = i, Y_1 = v_{j_1},..., Y_n = v_{j_n}, \lambda \} \\ 
            &= \alpha_n(i) P \{ X_{n+1} = j, Y_{n+1} = v_{j_{n+1}},..., Y_N = v_{j_N} | X_n = i, \lambda \} \\
            &= \alpha_n(i) a_{ij} P \{ Y_{n+1} = v_{j_{n+1}},..., Y_N = v_{j_N} | X_n = i, X_{n+1} = j, \lambda \} \\
            &= \alpha_n(i) a_{ij} b_{jj_{n+1}} P \{ Y_{n+2} = v_{j_{n+2}},..., Y_N = v_{j_N} | X_{n+1} = j, \lambda \} \\
            &= \alpha_n(i) a_{ij} b_{jj_{n+1}} \beta_{n+1} (j)
        \end{align}
    \end{proof}
    \subsection{}
    \begin{proof}
        \begin{align}
            d_{ij} &= E [ \sum_{n=0}^\infty f (X_n) | X_0 = j ] - E [ \sum_{n=0}^\infty f (X_n) | X_0 = i ] \\
            &= \mathop{\lim}_{N \rightarrow \infty} \{ E [ \sum_{n=0}^N f (X_n) | X_0 = j ] - E [ \sum_{n=0}^N f (X_n) | X_0 = i ] \} \\
            &= \mathop{\lim}_{N \rightarrow \infty} \sum_{n=0}^N \{ E [ f (X_n) | X_0 = j ] - E [ f (X_n) | X_0 = i ] \} \\
            &= \mathop{\lim}_{N \rightarrow \infty} \sum_{n=0}^N \{ (P^n)_j f - (P^n)_i f \} 
        \end{align}
        So that we have
        \begin{align}
            D = \mathop{\lim}_{N \rightarrow \infty} \sum_{n=0}^N \{ ef^\tau (P^n)^\tau - P^n f e^\tau \}
        \end{align}
        Then,
        \begin{align}
            &D - PDP^\tau \\
            &= \mathop{\lim}_{N \rightarrow \infty} \sum_{n=0}^N  \{ ef^\tau (P^n)^\tau - P^n f e^\tau - (Pef^\tau (P^n)^\tau P^\tau - PP^n f e^\tau P^\tau ) \} \\
            &= \mathop{\lim}_{N \rightarrow \infty} \sum_{n=0}^N  \{ ef^\tau ((P^n)^\tau - (P^{n+1})^\tau) - (P^n - P^{n+1}) fe^\tau \} \\
            &= \mathop{\lim}_{N \rightarrow \infty} ef^\tau ((P^0)^\tau - (P^{n+1})^\tau) - (P^0 - P^{n+1}) fe^\tau  \\
            &= \mathop{\lim}_{N \rightarrow \infty} ef^\tau (I - (P^{n+1})^\tau) - (I - P^{n+1}) fe^\tau \\
            &= ef^\tau (I - e \pi) - (I - pi^\tau e^\tau) fe^\tau \\
            &= ef^\tau - fe^\tau = F
        \end{align}
    \end{proof}
    \subsection{}
    \begin{proof}
        If $(I - P + e\pi)$ is invertible, then there exists a vector $y \neq 0$ so that
        $y (I - P + e\pi) = 0$.
        \begin{align}
            y(I - P + e\pi) &= 0 \\
            y(I - P + e\pi)e &= 0 \\
            ye - yPe + e\pi e &= 0 \\
            ye - ye + e &= 0
        \end{align}
        Conflict!
    \end{proof}
    \subsection{}
    \begin{proof}
        \par
        (1) Sufficient condition:
        \par
        If
        \begin{align}
            f^{v^*} + A^{v^*} g^{v^*} \leq f^v + A^v g^{v^*}, v \in \Omega
        \end{align}
        then we have
        \begin{align}
            f^{v^*} + A^{v^*} g^{v^*} - (f^v + A^v g^{v^*}) &\leq 0 \\
            p^v[f^{v^*} + A^{v^*} g^{v^*} - (f^v + A^v g^{v^*})] &\leq 0 \\
            \eta^{v^*} - \eta^v &\leq 0
        \end{align}
        that is, $v^*$ is an optimal policy.
        \par
        (2) Necessary condition:
        If $v^*$ is an optimal policy, then we have $\eta^{v^*} - \eta^v \leq 0, \forall v \in \Omega_s$.
        If (106) does not stand, then there exist a policy $u$, so that at state $i_0$, we have
        \begin{align}
            f^{v^*}(i_0) + A_{i_0}^{v^*} g^{v^*} > f^v(i_0) + A_{i_0}^v g^{v^*}
        \end{align}
        Then if we set policy $u'(i) = v^*(i), i \neq i_0$, at state $i_0$, $u'(i_0) = u(i_0)$.
        So apparently we have
        \begin{align}
            f^{v^*} + A^{v^*} g^{v^*} > f^{u'} + A^{u'} g^{v^*}
        \end{align}
        From the sufficient condition, we can see that $v^*$ is not an optimal policy which is conflict with
        our assumption.
    \end{proof}
    \subsection{}
    \begin{proof}
        \par
        (1) Sufficient condition: If
        \begin{align}
            0 = \mathop{\min}_{v \in \Omega_s} \{ f^v + A^v g^{v^*} - e \eta^{v^*} \}
        \end{align}
        we can get
        \begin{align}
            f^{v^*} + A^{v^*} g^{v^*} \leq f^v + A^v g^{v^*}, v \in \Omega
        \end{align}
        So from Theory 3.2 we know that $v^*$ is an optimal policy.
        \par
        (2) Necessary condition: If $v^*$ is an optimal policy, then from Theory 3.2
        we have
        \begin{align}
            f^{v^*} + A^{v^*} g^{v^*} \leq f^v + A^v g^{v^*}, v \in \Omega
        \end{align}
        that is
        \begin{align}
            0 \leq f^v + A^v g^{v^*} - e \eta^{v^*}, v \in \Omega_s
        \end{align}
        set $v = v^*$
        we have
        \begin{align}
            0 = f^v + A^v g^{v^*} - e \eta^{v^*}, v \in \Omega_s
        \end{align}
        that is
        \begin{align}
            0 = \mathop{\min}_{v \in \Omega_s} \{ f^v + A^v g^{v^*} - e \eta^{v^*} \}
        \end{align}
    \end{proof}
    \subsection{}
    \begin{proof}
        Notice that 
        \begin{align}
            U_\alpha = \int_0^\infty e^{-\alpha t} P(t)dt
        \end{align} exists, and $U_\alpha > 0$.
        \par
        Due to
        \begin{align}
            P_{ij}(t) = \int_0^t h(j, t - s) R_{ij}(s)ds
        \end{align}
        We have
        \begin{align}
            [U_\alpha]_{ij} &= \int_0^\infty e^{-\alpha t} P_{ij}(t)dt \\
            &= \int_0^\infty e^{-\alpha t} \int_0^t h(j, t - s) R_{ij}(s)dsdt \\
            &= \int_0^\infty \int_0^t e^{-\alpha t} h(j, t - s) R_{ij}(s)dsdt \\
            &= \int_0^\infty \int_s^\infty e^{-\alpha t} h(j, t - s) R_{ij}(s)dtds \\
            &= \int_0^\infty e^{-\alpha s} \int_s^\infty e^{-\alpha (t - s)} h(j, t - s) R_{ij}(s)dtds \\
            &= \int_0^\infty e^{-\alpha s} \int_0^\infty e^{-\alpha \tau} h(j, \tau) R_{ij}(s)d\tau ds \\
            &= \int_0^\infty e^{-\alpha s} h_\alpha (j) R_{ij}(s) ds \\
            &= h_\alpha (j) [R_\alpha]_{ij} 
        \end{align}
        So that we have
        \begin{align}
            U_\alpha = R_\alpha H_\alpha = (I - Q_\alpha)^{-1} H_\alpha
        \end{align}
        Then we have
        \begin{align}
            U_\alpha (\alpha I - A_\alpha) = (I - Q_\alpha)^{-1} H_\alpha H_\alpha^{-1}(I - Q_\alpha) = I
        \end{align}
    \end{proof}
    \subsection{}
    \begin{proof}
        Notice that
        \begin{align}
            [U_\alpha]_i e = \sum_{j=1}^K \int_0^\infty e^{-\alpha t} P_{ij}(t)dt = \int_0^\infty e^{-\alpha t} dt = \frac{e}{\alpha}
        \end{align}
        From 0.17, we have
        \begin{align}
            \alpha I - A_\alpha = U_\alpha^{-1}
        \end{align}
        then we have for $\alpha > 0$,
        \begin{align}
            &( U_\alpha - \frac{ep_\alpha}{\alpha(1 - \alpha)}) ( U_\alpha^{-1} + ep_\alpha) \\
            &= I - \frac{ep_\alpha U_\alpha^{-1}}{\alpha(1 + \alpha)} + \frac{ep_\alpha}{\alpha} - \frac{ep_\alpha}{\alpha(1+\alpha)} \\
            &= I - \frac{ep_\alpha (\alpha I - A_\alpha) - \alpha ep_\alpha }{\alpha(1 + \alpha)} \\
            &= I - \frac{ep_\alpha A_\alpha }{\alpha(1 + \alpha)} \\
            &= I
        \end{align}
        when $\alpha = 0$, we need to prove that $A + ep$ is invertible.
        \par
        If $A + ep$ is not invertible, then there exists a vector $y \neq 0$
        so that $y(A+ep) = 0$.
        \begin{align}
            y(A+ep) &= 0 \\
            yAe + yepe &= 0 \\
            ye &= 0
        \end{align}
        then we have $yA = 0$ as well.
        Notice that from equation $pe = 1, pA = 0$ we can get the only solution
        $p$, so that without equation $ye = 1$ we can get $y$ must satisfies
        $y = cp, c \neq 0$, which is conflict with $ye = 0$.

    \end{proof}
    \subsection{}
    \begin{proof}
        \begin{align}
            (\alpha I - A_\alpha) g_\alpha &= f - \frac{ep_\alpha f}{1 + \alpha} \\
            g_\alpha &= U_\alpha (f - \frac{ep_\alpha f}{1 + \alpha}) \\
            g_\alpha &= \eta_\alpha - \frac{ep_\alpha f}{\alpha (1 + \alpha)} \\
            \eta_\alpha &= g_\alpha + \frac{ep_\alpha f}{\alpha (1 + \alpha)}
        \end{align}
    \end{proof}
    \subsection{}
    \end{document}