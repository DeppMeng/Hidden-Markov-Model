% last updated in April 2002 by Antje Endemann
% Based on CVPR 07 and LNCS, with modifications by DAF, AZ and elle, 2008 and AA, 2010, and CC, 2011; TT, 2014; AAS, 2016

\documentclass[runningheads]{llncs}
    \usepackage{graphicx}
    \usepackage{amsmath,amssymb} % define this before the line numbering.
    \usepackage{ruler}
    \usepackage{color}
    \usepackage[width=122mm,left=12mm,paperwidth=146mm,height=193mm,top=12mm,paperheight=217mm]{geometry}
    \usepackage{multirow, multicol}
        
    % \newtheorem{theorem}{Theorem}[section]
    % \newtheorem{lemma}[theorem]{Lemma}
    % \newtheorem{definition}{Definition}[section]
    
    \begin{document}
    % \renewcommand\thelinenumber{\color[rgb]{0.2,0.5,0.8}\normalfont\sffamily\scriptsize\arabic{linenumber}\color[rgb]{0,0,0}}
    % \renewcommand\makeLineNumber {\hss\thelinenumber\ \hspace{6mm} \rlap{\hskip\textwidth\ \hspace{6.5mm}\thelinenumber}}
    % \linenumbers
    \pagestyle{headings}
    \mainmatter
    \def\ECCV18SubNumber{1452}  % Insert your submission number here
    
    \title{Markov Decision Process} % Replace with your title
    
    \titlerunning{ECCV-18 submission ID \ECCV18SubNumber}
    
    \authorrunning{ECCV-18 submission ID \ECCV18SubNumber}
    
    \author{Depu Meng}
    \institute{Oct. $2018$}
    
    
    \maketitle
    \section{Basic Concepts \& Examples}
    \emph{Markov Decision Process} (MDP) is a Markov Process that decisions are involved.
    Generally, an MDP can described by a quintuple:
    \begin{itemize}
        \item A Markov Process or an Extended Markov Process to describe the process.
        \item A state space.
        \item An action space.
        \item A state transition function.
        \item A performance function.
    \end{itemize}

    MDP can be divided into Continuous-Time MDP and Discrete-Time MDP
    according to the time factor of the Markov Process;
    MDP can also be divided into MDP and Semi-MDP and Partially-Observable MDP.

    \subsection{Policy and policy space}
    In this section, we will take policy and policy space in DTMDP as an example.
    \par
    A DTMDP quintuple can be denoted as $\{ X, \Phi, A, P_{ij}(a), f(i, a) \}$,
    \par\noindent
    $X = \{ X_n; n \geq 0 \}$ is a Discrete-Time Markov Process, $\Phi = \{i \}$
    and $A = \{ a \}$ are state space and action space of this process respectively.
    For $P_{ij}(a)$, appearently we have $P_{ij}(a) \geq 0$ and
    $\sum_{j \in \Phi} P_{ij}(a) = 1$.
    A DTMDP sample orbit can be described as $\{ i_0, a_0, i_1, a_1,...\}$.
    Denote $h_n = \{ i_0, a_0,...,i_{n-1}, a_{n-1}, i_n \}$ as the history
    before time $n$.
    \par
    A general policy is defined as
    \begin{align}
        v = (v_0(a | h_0), v_1(a | h_1),...)    
    \end{align}
    Infact, a general policy is a series of action defined on decision time,
    which is also a stochastic policy if not specified.
    The set that contains all policies like (1) is a policy space, denoted as $\Pi$.
    \par
    For a policy, if for each $v_n(a|h_n)$, we select action $a$ w.p.1, then we call it
    a determined policy, all determined polices is denoted as $\Pi^d$.
    \par
    For a policy, if each $v_n(a|h_n)$ only related to initial state $i_0$ and state
    of time $n$ $i_n$, i.e., for any $n$, we have $v_n(a|h_n) = v_n(a|i_0, i_n)$,
    then we call it a semi-Markov policy, denoted as $\Pi_{sm}$.
    Similarily, we can define Markov policy $\Pi_m$.
    \par
    For a Markov policy, if it is also determined policy, and $v_n(a|i_n) = v(a|i)$,
    then we call it determined steady Markov policy $\Pi_s^d$,
    which is a mapping from state space to action space, i.e., $v: \Phi \rightarrow A$.
    \par
    If not specified, we only need to find the optimal policy in the determined steady
    policy set.

    \subsection{Performance evaluation}
    \subsubsection{Performance evaluations of DTMDP}
    DTMDP performance evaluations can be devided into the following three classes.
    \begin{align}
        \eta_N^v(i) = \sum_{n=0}^N E_v \{ f(X_n, v(X_n)) | X_0 = i \}, i \in \Phi
    \end{align}
    where $E_v$ means the expectation over policy $v \in \Pi$. This is called \emph{finite
    time performance evaluation}.
    \begin{align}
        \eta_\alpha^v(i) = E_v \{ \sum_{n=0}^\infty \alpha^n f(X_n, v(X_n)) | X_0 = i \}, i \in \Phi
    \end{align}
    where $\alpha$ is called the discount factor. This is called \emph{infinite time
    discounted performance evaluation}.
    \begin{align}
        \eta^v(i) = \mathop{\lim}_{N\rightarrow\infty} \frac{1}{N} E_v \{ \sum_{n=0}^{N-1} f(X_n, v(X_n)) | X_0 = i \}, i \in \Phi
    \end{align}
    This is called \emph{infinite time average performance evaluation}.

    \subsubsection{Performance evaluations of CTMDP}
    Firstly we show the quintuple representation of a CTMDP.
    A CTMDP can be represented as $\{ Y, \Phi, A, a_{ij}(a), f(i, a) \}$,
    $Y = \{ Y_t; t \geq 0 \}$ is a continuous-time Markov process with
    state space $\Phi = \{ i \}$ and action space $A = \{a\}$.
    The transition speed $a_{ij}(a)$ satisfies for any $i, j \in \Phi, i \neq j, a \in A$,
    $a_{ij}(a) \geq 0, a_{ii}(a) \leq 0$ and $\sum_{j \in \Phi} a_{ij}(a) = 0$.

    \par
    In this study, we assume that the decision point is always the time when state transits,
    then we can have a similar definition as DTMDP.
    \begin{align}
        \eta_\alpha^v(i) = E_v \{ \int_{0}^\infty e^{-\alpha t} f(Y_t, v(Y_t)) dt | Y_0 = i \}, i \in \Phi
    \end{align}
    We call it \emph{infinite time
    discounted performance evaluation}.
    \begin{align}
        \eta^v(i) = \mathop{\lim}_{T\rightarrow\infty} \frac{1}{T} E_v \{ \int_{0}^{T} f(Y_t, v(Y_t)) dt | Y_0 = i \}, i \in \Phi
    \end{align}
    This is called \emph{infinite time average performance evaluation}.
    \par
    If the Markov process Y is ergodic, then $\eta^v(i)$ is irrelevant to
    initial state, so we have
    \begin{align}
        \eta^v = \sum_{i \in \Phi} p^v (i) f(i, v(i)) = p^v f^v
    \end{align}
    where $p^v(i), i \in \Phi$ is the steady distribution of Markov process $Y$
    with policy $v$. We call $\eta^v$
    the performance measure under policy $v$.
    \par
    One objective of MDP is to find $v^* \in \Pi$ so that all performance is optimal.

    \section{DTMDP}
    \subsection{Performance potential}
    For an ergodic Markov Chain $X = \{ X_n; n \geq 0 \}$, its state space $\Phi = \{ 1, 2,..., K \}$,
    finite action space $A$, policy space determined steady policy set $\Pi_s^d$,
    with policy $v \in \Pi_s^d$, its transition matrix $P^v = [P_{ij}(v(i))]$,
    steady distribution $\pi^v = (\pi^v(1),..., \pi^v(K))$ satisfies
    \begin{align}
        \pi^v P^v = \pi^v, \pi^v e = 1
    \end{align}
    where $e = (1, 1,..., 1)^\tau$.
    \par
    We only consider the infinite time average performance evaluation.
    \begin{align}
        \eta^v = \sum_{n=0}^K \pi^v f(i, v(i)) = \pi^v f^v
    \end{align}
    where $f^v = (f(1, v(1)),...,f(K, v(K))^\tau$.
    \par
    Denote 
    \begin{align}
        g^v(i) = E \{ \sum_{n=0}^\infty [f(X_n, v(X_n)) - \eta^v] | X_0 = i \}, i = \Phi
    \end{align}
    \begin{definition}
        $g^v = (g^v(1),..., g^v(K))^\tau$ is called performance potential vector,
    or potential of Markov Chain $X$ w.r.t performance function $f^v$ at state $i$.
    \end{definition} 
    \begin{lemma}
    potential $g$ satisfies Poisson equation
    \begin{align}
        (I - P^v)g^v = f^v - \eta^v e
    \end{align}
    all potentials can be represented as 
    \begin{align}
        g^v = (I - P^v + e \pi)^{-1} f^v + ce
    \end{align}
    where $c$ can be any constant.
    \end{lemma}
    \begin{proof}
    \begin{align}
        g(i) &= \mathop{\lim}_{N \rightarrow \infty} E \{ \sum_{n=0}^N [f(X_n, v(X_n)) - \eta^v] | X_0 = i \} \\
        &= \mathop{\lim}_{N \rightarrow \infty}  \sum_{n=0}^N E [f(X_n, v(X_n)) - \eta^v | X_0 = i ] \\
        &= f(i, v(i)) - \eta^v + \mathop{\lim}_{N \rightarrow \infty} \sum_{n=1}^N E [f(X_n, v(X_n)) - \eta^v | X_0 = i ] \\
        &= f(i, v(i)) - \eta^v + \sum_{j \in \Phi} P^v_{ij} \mathop{\lim}_{N \rightarrow \infty} \sum_{n=1}^N E [f(X_n, v(X_n)) - \eta^v | X_1 = j ] \\
        &= f(i, v(i)) - \eta^v + \sum_{j \in \Phi} P^v_{ij} g^v(j)
    \end{align}
    that is,
    \begin{align}
        g^v &= f^v - \eta^v e + P^v g^v \\
        (I - P^v)g^v &= f^v - \eta^v e
    \end{align}
    \par
    Appearently $rank (I - P^v) = K - 1$, so that solution space of equation $(I - P^v) g^v$ is 1-dimensional
    and $g^v = e$ is one solution. Then we need a particular solution of the equation that satisfies $\pi g = \eta$.
    That is 
    \begin{align}
        g^v = (I - P^v + e\pi)^{-1}f^v
    \end{align} 
    \end{proof}

    Theoretically performance potential can be solved from Poisson equation,
    but when state space is too large, it is not very easy to be solved.
    So we often use sample orbit to estimate it. Denote
    \begin{align}
        g_L(i) = E[\sum_{l=0}^{L-1}f(X_l)|X_0=i] - L\eta
    \end{align}
    $g(i) = \mathop{\lim}_{L \rightarrow \infty} g_L(i)$, because of the property of potential,
    we can ignore constant term $L\eta$.
    \begin{align}
        g_L(i) \approx E[\sum_{l=0}^{L-1}f(X_l)|X_0=i]
    \end{align}
    denote
    \begin{align}
        g_{L,N}(i) = \frac{\sum_{n=0}^{N-L+1} I_i(X_n) [\sum_{l=0}^{L-1}f(X_l)|X_0=i]}{\sum_{n=0}^{N-L+1} I_i(X_n)}
    \end{align}
    where $I_i(x)$ is characteristic function of state $i$, so we have
    \begin{align}
        g_L(i) = \mathop{\lim}_{N\rightarrow \infty} g_{L,N}(i), (w.p.1)
    \end{align}
    \subsection{Performance optimization}
    \subsubsection{Performance difference}
    Denote $\eta^u, \pi^u, P^u, g^u$ and $\eta^v, \pi^v, P^v, g^v$
    are values under policy $u$ and $v$ respectively,
    from Poisson equation, we have
    \begin{align}
        (I - P^u)g^u &= f^u - \eta^u e \\
        (\pi^v - \pi^vP^u)g^u &= \pi^v f^u - \pi^v\eta^u e \\
        \eta^u &= \pi^v f^u - (\pi^v - \pi^vP^u)g^u \\
        \eta^u &= \pi^v f^u - (\pi^v P^v - \pi^vP^u)g^u \\
        \eta^u &= \pi^v f^u - \pi^v (P^v - P^u)g^u \\
    \end{align} 
    with $\eta^v = \pi^v f^v$, we have performance difference formula
    \begin{align}
        \eta^u - \eta^v &= \pi^v f^u - \pi^v (P^v - P^u)g^u - \pi^v f^v \\
        &= \pi^v [(P^u g^u + f^u) - (P^v g^u + f^v)]
    \end{align}
    for $x = \{ x_1,...,x_K \}$ and $y = \{ y_1,...,y_K \}$, $x \prec y$
    means for any $i$, we have $x_i \leq y_i$ and there exists at least one
    $j, 1 \leq j \leq K$ satisfies $x_j < y_j$.
    \begin{lemma}
    (1) If
    \begin{align}
        P^ug^u + f^u \prec P^vg^u + f^v
    \end{align}
    then $\eta^u < \eta^v$.
    \par
    (2) $v^*$ is the optimal policy of infinite time average performance
    if and only if for any $v \in \Pi_s^d$,
    \begin{align}
        P^{v^*}g^{v^*} + f^{v^*} \leq P^vg^{v^*} + f^v
    \end{align}
    we call it optimal inequality.
    \end{lemma}
    \begin{proof}
    If (34) holds for $\forall v \in \Pi_s^d$, then
    \begin{align}
        P^{v^*}g^{v^*} + f^{v^*} &\leq P^vg^{v^*} + f^v \\
        \pi^v \{P^{v^*}g^{v^*} + f^{v^*} - (P^vg^{v^*} + f^v) \} &\leq 0\\
        \eta^{v^*} - \eta^v &\leq 0\\
    \end{align}
    so that $v^*$ is an optimal policy.
    \par
    On the other hand, if $v^*$ is a optimal policy, then for any $v \in \Pi_s^d$,
    we have $\eta^{v^*} \leq \eta^v$.
    If (34) is not satisfied, then there exists a policy $u \in \Pi_s^d$,
    \begin{align}
        \sum_{j \in \Phi} P_{i_0 j} (u(i_0)) g^{v^*}(j) + f(i_0, u(i_0)) < \sum_{j \in \Phi} P_{i_0 j} (v^*(i_0)) g^{v^*}(j) + f(i_0, v^*(i_0))
    \end{align}
    then we can design $w \in \Phi_s^d$, let $w(i) = v^*(i), i \neq i_0$,
    $w(i_0) = u(i_0)$, then we can get $\eta^w < \eta^{v^*}$, which is conflict to the assumption.
    \end{proof}
    \begin{theorem}
        $v^*$ is the optimal policy for infinite time average performance
        if and only if $v^*$ satisfies
        \begin{align}
            0 = \mathop{\min}_{v \in \Pi_s^d} \{ f^v + (P^v - I)g^{v^*} - \eta^{v^*}e \}
        \end{align}
    \end{theorem}
    \begin{proof}
        Hint: Combine (34) and (11).
    \end{proof} 
    \paragraph{Algorithm.}
    In policy iteration, we select
    \begin{align}
        v_{k+1}(i) = \mathop{\arg\min}_{v(i)\in A(i)} \sum_{j=1}^K P_{ij} (v(i))g^{v_k}(i) + f(i, v(i))
    \end{align}
    \subsubsection{Performance derivative}
    If both transition matrix and performance function is related to
    parameter $\theta \in I$, then we can consider the
    \emph{parameterizing policy.}
    Assume $P(\theta), f(\theta)$ are differentiable functions.
    From Poisson equation (11),
    \begin{align}
        - \frac{\partial P}{\partial \theta} g + (I - P) \frac{\partial g}{\partial \theta} = \frac{\partial f}{\partial \theta} - \frac{\partial \eta}{\partial \theta}e
    \end{align}
    notice that $\pi P =  \pi, \pi e = 1$, we have
    \begin{align}
        - \pi \frac{\partial P}{\partial \theta} g + \pi (I - P) \frac{\partial g}{\partial \theta} &= \pi\frac{\partial f}{\partial \theta} - \pi\frac{\partial \eta}{\partial \theta}e \\
        - \pi \frac{\partial P}{\partial \theta} g &= \pi\frac{\partial f}{\partial \theta} - \frac{\partial \eta}{\partial \theta} \\
       \frac{\partial \eta}{\partial \theta}  &= \pi\frac{\partial f}{\partial \theta} + \pi \frac{\partial P}{\partial \theta} g
    \end{align}
    For $\theta = (\theta_1,...,\theta_m)$, we have
    \begin{align}
        \triangledown \eta = \Big( \frac{\partial \eta}{\partial \theta_1}, \frac{\partial \eta}{\partial \theta_2},..., \frac{\partial \eta}{\partial \theta_m} \Big)
    \end{align} 
    Any optimal policy $\theta^*$ should satisfies
    \begin{align}
        \theta^* = \mathop{\arg}_{\theta} \{ \triangledown \eta = 0 \}
    \end{align}

    \section{CTMDP}
    \subsection{Realization matrix and performance potential}
    \subsubsection{Performance measure}
    Consider a ergodic Markov process $Y = \{ Y_t; t \geq 0 \}$ with
    state space $\Phi = \{ 1, 2,..., K \}$ and infinitesimal generator
    matrix $A = [a_{ij}]$. $Y$ has unique steady distribution
    $p = (p(1),..., p(K))$, and $p(i) > 0, i \in \Phi$. We have,
    \begin{align}
        pe = 1, Ae = 0, pA = 0
    \end{align}
    steady performance measure $\eta$ is the expectation of $f$ w.r.t $p$
    \begin{align}
        \eta = \sum_{i = 1}^K p(i) f(i) = pf
    \end{align}
    which is also the infinite time average performance measure.
    \subsubsection{Realization matrix and performance potential}
    Firstly we introduce the concept of \emph{group inverse of infinitesimal generator matrix}.
    \begin{definition}
        \begin{align}
            A_p = \{ A: pA = 0, Ae = 0 \}
        \end{align}
        $A_p$ is a group that has multiplication identity element $I-ep$.
    \end{definition}
    \begin{definition}
        Inverse of $A$ in group $A_p$ is called the group inverse of $A$, denoted as $A^\#$
    \end{definition}
    \begin{lemma}
        $A + ep$ is invertible.
    \end{lemma} 
    \begin{proof}
        If $A + ep$ is not invertible, then there exists a vector $y \neq 0$
        so that $y(A+ep) = 0$.
        \begin{align}
            y(A+ep) &= 0 \\
            yAe + yepe &= 0 \\
            ye &= 0
        \end{align}
        then we have $yA = 0$ as well.
        Notice that from equation $pe = 1, pA = 0$ we can get the only solution
        $p$, so that without equation $ye = 1$ we can get $y$ must satisfies
        $y = cp, c \neq 0$, which is conflict with $ye = 0$.
    \end{proof}
    \begin{lemma}
        Group inverse of $A$ is
        \begin{align}
            A^\# = (A + ep)^{-1} - ep
        \end{align}
    \end{lemma}
    \begin{proof}
        \begin{align}
            A[(A+ep)^{-1} - ep] &= I - ep \\
            A[(A+ep)^{-1} - ep](A + ep) &= (I - ep)(A + ep) \\
            A - AepA - Aepep &= A + ep - epA - epep \\
            A &= A + ep - ep 
        \end{align}
        \end{proof}
    \par
    For any $i, j \in \Phi$, let $Y^{i} = \{ Y_t: Y_0 = i; t \geq 0 \}$ and $Y^{j} = \{ Y_t: Y_0 = j; t \geq 0 \}$,
    for any $i, j \in \Phi$, $Y^i$ and $Y^j$ are independent.
    Define $Z^{i,j} = (Y^i, Y^j)$, then $Z^{i, j}$ is a Markov process that has
    state space $\Phi \times \Phi$. Appearently $Z^{i,j}$ is ergodic.
    Denote $S = \{ (k, k): k \in \Phi \}$,
    we define $T^{ij} = \mathop{\inf} \{ t: t \geq 0, Z^{ij}_t \in S \}$.
    
    \begin{definition}
        We call $d_{ij} = E \{ \int_0^{T^{ij}} [ f(Y_t^j) - f(Y_t^i) ] dt \}, i, j \in \Phi$
        the realization factor of pertubation of Markov process $Y$ w.r.t performance function $f$.
        Matrix $D = [d_{ij}$] is the corresponding realization matrix.
    \end{definition}

    \begin{lemma}
        \begin{align}
            d_{ij} = \mathop{\lim}_{T \rightarrow \infty} \{ E [ \int_0^T f(Y_t^j)dt ] -  E [ \int_0^T f(Y_t^i)dt ] \}. i, j \in \Phi
        \end{align}
    \end{lemma}
    \par
    From (59) we can get
    \begin{align}
        d_{ij} = d_{ik} + d_{kj}, i, j, k \in \Phi
    \end{align}
    

    \end{document}