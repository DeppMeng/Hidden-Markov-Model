% last updated in April 2002 by Antje Endemann
% Based on CVPR 07 and LNCS, with modifications by DAF, AZ and elle, 2008 and AA, 2010, and CC, 2011; TT, 2014; AAS, 2016

\documentclass[runningheads]{llncs}
\usepackage{graphicx}
\usepackage{amsmath,amssymb} % define this before the line numbering.
\usepackage{ruler}
\usepackage{color}
\usepackage[width=122mm,left=12mm,paperwidth=146mm,height=193mm,top=12mm,paperheight=217mm]{geometry}
\usepackage{multirow, multicol}
    
% \newtheorem{theorem}{Theorem}

\begin{document}
% \renewcommand\thelinenumber{\color[rgb]{0.2,0.5,0.8}\normalfont\sffamily\scriptsize\arabic{linenumber}\color[rgb]{0,0,0}}
% \renewcommand\makeLineNumber {\hss\thelinenumber\ \hspace{6mm} \rlap{\hskip\textwidth\ \hspace{6.5mm}\thelinenumber}}
% \linenumbers
\pagestyle{headings}
\mainmatter
\def\ECCV18SubNumber{1452}  % Insert your submission number here

\title{Hidden Markov Model} % Replace with your title

\titlerunning{ECCV-18 submission ID \ECCV18SubNumber}

\authorrunning{ECCV-18 submission ID \ECCV18SubNumber}

\author{Depu Meng}
\institute{Oct. $2018$}


\maketitle
\section{Basic Concepts \& Examples}
\subsection{Basic Concepts}
\paragraph{Definition 1.1}
Assume $X = \{ X_n; n \geq 1 \} $ is a Markov chain over finite state space
$\Phi = \{ 1, 2, ...,K \} $, if states of $X$ are unobservable,
$Y = \{ Y_n; n \geq 1 \} $ is an observable random variable series over
finite set $V = \{ v_1, v_2,...,v_L \} $ that is correlated with $X$,
then $(X, Y) = \{ (X_n, Y_n); n \geq 1 \} $ is a \emph{Hidden Markov Chain}.
\par
Denote $\pi = \{ \pi_1, \pi_2,...,\pi_k \} $ is the initial distribution of $X$,
$A = [a_{ij}]$ is the transition matrix of $X$
\begin{align}
    a_{ij} = P\{X_{n+1} = j | X_n = i\}, i, j \in \Phi
\end{align}
is the one-step transition probality of X. Denote
\begin{align}
    b_{ij} = P\{ Y_n = v_j | X_n = i \}, i \in \Phi, v_j \in V
\end{align}
represents the probability that $Y$ equals to $v_j$ given $X$ is at state $i$ at time $n$.
If $X$ is homogeneous, then $Y$ is also irrelavent with time $n$.
Denote $B = [b_{ij}]$ as the observation probability matrix.
Due to the unobservability of $X$, $\pi, A, B$ can not be directly measured.
Generally, we call parameter set $\lambda = \{ \pi, A, B \} $
the math model of Hidden Markov chain $(X, Y)$, as well as \emph{Hidden Markov Model} (HMM).
\paragraph{Property 1.1}
Assume $N$ is the length of time on observation,
denote $\mathbf{X}= \{ X_1, X_2,..., X_N \} $, $\mathbf{Y} = \{ Y_1, Y_2,..., Y_N \} $
as the sample series of Markov chain $X$ and observation series $Y$ in the time period $1\sim N$
respectively, then the joint distribution of $\mathbf{X}$ and $\mathbf{Y}$ satisfy
the following \emph{Hidden Markov Condition}.
\begin{align}
    P\{ \mathbf{X} = x, \mathbf{Y} = y \} = \pi_{i_1} b_{i_1 j_1}... b_{i_{N-1} j_{N-1}} b_{i_{N-1} j_{N}} b_{i_{N} j_{N}},
\end{align}
where $x = \{ i_1, ..., i_N \} $, $y = \{ v_{j_1}, ..., v_{j_N} \} $.

\subsection{Examples}
\paragraph{Coin flipping problem}
Stochasticly choose a coin from two coins indexed $1 and 2$, and then toss it,
observe the result and repeat this process.
Denote $X_n$ as the coin chosen in $t^{th}$ time,
then $X = \{ X_n; n \geq 1 \} $ is a Markov chain over state space
$\Phi = \{ 1, 2 \} $.
Denote $Y_n$ as the result of $n^{th}$ experiment,
then $Y = \{ Y_n; n \geq 1\} $ is observation series
that takes values from $V =  \{ H, T \} $, where $H$ means the front side and $T$
means the back side.
State transition matrix and observation probability matrix are
\begin{align}
    A =
    \begin{bmatrix}
        a_{11} &a_{12} \\
        a_{21} &a_{22} \\
    \end{bmatrix},
    B =
    \begin{bmatrix}
        b_{11} &b_{12} \\
        b_{21} &b_{22} \\
    \end{bmatrix},
\end{align}
where summation of each row equals to $1$.
For initial distribution $\pi = \{ \pi_1, \pi_2 \} $,
we have $\pi_1 + \pi_2 = 1$.
Then $\lambda  =\{ \pi, A, B \} $ is the corresponding HMM of Hidden Markov chain $(X, Y)$,
which contains $5$ unknown parameters.

\section{Basic Problems \& Solutions}
\subsection{Basic Problems}
\subsubsection{Problem 1}
For a specific observation sample series $\mathbf{Y} = \{ Y_1, ..., Y_n \} $,
known it is generated by one of the given HMMs, determine which model that generates the 
sample series is called \emph{Pattern Recognition} problem, as well as \emph{classification} problem.
\subsubsection{Problem 2}
From a series of observation samples $\mathbf{Y} = \{ Y_1, ..., Y_n \} $ and known HMM
$\lambda = \{ \pi, A, B \} $, give the best estimate of hidden states is called \emph{State Estimate}
problem, as well as \emph{Decoding} problem.
\subsubsection{Problem 3}
From a series of observation samples $\mathbf{Y} = \{ Y_1, ..., Y_n \} $,
give the best estimate of parameter set $\lambda = \{ \pi, A, B \} $,
we call it \emph{Model Learning} problem.

\subsection{Solutions}

\subsubsection{Solution to Problem 1}
To solve Problem 1 is mainly to solve $P \{ \mathbf{Y} | \lambda \} $,
for different $\lambda$, we can compare with their corresponding
$P \{ \mathbf{Y} | \lambda \} $, and choose the largest one.
So how to compute $P \{ \mathbf{Y} | \lambda \} $?
\begin{align}
    P \{ \mathbf{Y} = y | \lambda \}
    &= \sum_x P \{ \mathbf{Y} = y, \mathbf{X} = x | \lambda \} \\
    &= \sum_x \pi_{i_1} b_{i_1 j_1}... b_{i_{N-1} j_{N-1}} b_{i_{N-1} j_{N}} b_{i_{N} j_{N}}
\end{align}
\par
In order to make the writing more convenient, we introduce notation
$\alpha_n(i)$ and $\beta_n(i)$ first.
\begin{align}
    &\alpha_n(i) = P \{ Y_1 = v_{j_1},..., Y_n = v_{j_n}, X_n = i | \lambda \} \\
    &\beta_n(i) = P \{ Y_{n+1} = v_{j_{n+1}},...,Y_N = v_{j_N} | X_n = i, \lambda \}
\end{align}
\par
Recurrence relation of $\alpha_n(i)$.
\begin{align}
    \alpha_n(i) = \sum_{j=1}^K \alpha_n(j) a_{ji} b_{ij_{n+1}}, n = 1,..., N - 1
\end{align}
\paragraph{proof}
\begin{align}
    &\alpha_{n+1}(i) 
    = P \{ Y_1 = v_{j_1}, Y_n = v_{j_n}, Y_{n+1} = v_{j_{n+1}}, X_{n+1} = i | \lambda \} \\
    &= \sum_{k=1}^K P \{ Y_1 = v_{j_1},..., Y_n = v_{j_n}, Y_{n+1} = v_{j_{n+1}}, X_{n} = k, X_{n+1} = i | \lambda \} \\
    &= \sum_{k=1}^K P_A P_B P_C
\end{align}
where
\begin{align}
    &P_A = P \{ Y_1 = v_{j_1},..., Y_n = v_{j_n}, X_{n} = k | \lambda \} \\
    &P_B = P \{ X_{n+1} = i | Y_1 = v_{j_1},..., Y_n = v_{j_n}, X_{n} = k, \lambda \} \\ 
    &P_C = P \{ Y_{n+1} = v_{j_{n+1}} | Y_1 = v_{j_1},..., Y_n = v_{j_n}, X_{n} = k, X_{n+1} = i, \lambda\}
\end{align}
Notice that for $P_A$, we have
\begin{align}
    P_A = P \{ Y_1 = v_{j_1},..., Y_n = v_{j_n}, X_{n} = k | \lambda \} = \alpha_n(k) \\
\end{align}
For $P_B$, we have
\begin{align}
    P_B &= P \{ X_{n+1} = i | Y_1 = v_{j_1},..., Y_n = v_{j_n}, X_{n} = k, \lambda \} \\
    &= P \{ X_{n+1} = i | X_{n} = k, \lambda \} \\
    &= a_{ki}
\end{align}
For $P_C$, we have
\begin{align}
    P_C &= P \{ Y_{n+1} = v_{j_{n+1}} | Y_1 = v_{j_1},..., Y_n = v_{j_n}, X_{n} = k, X_{n+1} = i, \lambda\} \\
    &= P \{ Y_{n+1} = v_{j_{n+1}} | X_{n+1} = i, \lambda\} \\
    &= b_{ij_{n+1}}
\end{align}
Combine the three equations together, we have
\begin{align}
    \alpha_{n+1}(i) &= \sum_{k=1}^K P_A P_B P_C \\
    &= \sum_{k=1}^K \alpha_n(k) a_{ki} b_{ij_{n+1}}
\end{align} \qed

\par
Recurrence relation of $\beta_n(i)$
\begin{align}
    \beta_n(i) = \sum_{j=1}^K \beta_{n+1}(j) a_{ij} b_{jj_{n+1}}, n = 1,...,N - 1
\end{align}
\emph{proof}
\begin{align}
    \beta_{n}(i) &= P \{ Y_{n+1} = v_{j_{n+1}},...,Y_N = v_{j_N} | X_n = i, \lambda \} \\
    &= \sum_{k=1}^K P \{ Y_{n+1} = v_{j_{n+1}},...,Y_N = v_{j_N}, X_{n+1} = k | X_n = i, \lambda \} \\
    &= \sum_{k=1}^K P_X P_Y P_Z
\end{align}
where
\begin{align}
    &P_X = P \{ X_{n+1} = k | X_n = i, \lambda \} \\
    &P_Y = P \{ Y_{n+2} = v_{j_{n+1}},..., Y_N = v_{j_n} | X_{n+1} = k, X_n = i, \lambda \} \\
    &P_Z = P \{ Y_{n+1} = v_{j_{n+1}} | X_{n+1} = k, X_n = i, Y_{n+2} = v_{j_{n+1}},..., Y_N = v_{j_n}, \lambda \}
\end{align}
For $P_X$, we have
\begin{align}
    P_X = P \{ X_{n+1} = k | X_n = i, \lambda \} = a_{ik}
\end{align}
For $P_Y$, we have
\begin{align}
    P_Y &= P \{ Y_{n+2} = v_{j_{n+1}},..., Y_N = v_{j_n} | X_{n+1} = k, X_n = i, \lambda \} \\
    &= P \{ Y_{n+2} = v_{j_{n+1}},..., Y_N = v_{j_n} | X_{n+1} = k, \lambda \} \\
    &= \beta_{n+1}(k)
\end{align}
For $P_Z$, we have
\begin{align}
    P_Z &= P \{ Y_{n+1} = v_{j_{n+1}} | X_{n+1} = k, X_n = i, Y_{n+2} = v_{j_{n+1}},..., Y_N = v_{j_n}, \lambda \} \\
    &= P \{ Y_{n+1} = v_{j_{n+1}} | X_{n+1} = k, \lambda \} \\
    &= b_{kj_{n+1}}
\end{align}
Combine the three equations, we have
\begin{align}
    \beta_n(i) &= \sum_{k=1}^K P_X P_Y P_Z \\
    &= \sum_{k=1}^K \beta_{n+1}(k) a_{ik} b_{kj_{n+1}}
\end{align} \qed
\par
From the $\alpha_n(i), \beta_n(i)$ defined above, we have
\begin{align}
    &P \{ \mathbf{Y} = y | \lambda \} = \sum_{j=1}^K \alpha_N(j) \\
    &P \{ \mathbf{Y} = y | \lambda \} = \sum_{j=1}^K \beta_1(j) \pi_j b_{jj_1} \\
\end{align}

\par
If we divide observation series $\mathbf{Y} = \{ Y_1,..., Y_N \}$
into $\{ Y_1,..., Y_n \}$ and \\ $\{ Y_{n+1},..., Y_N \}$, then we have
\begin{align}
    P \{ \mathbf{Y} = y, X_n = i | \lambda \} = \alpha_n(i) \beta_n(i)
\end{align}

\subsubsection{Solution to Problem 2}
For $n = 1, 2,..., N$, denote
\begin{align}
    \gamma_n(i) = P \{ X_n = i | Y_1 = v_{j_1}, ..., Y_N, \lambda \}
\end{align}
as the probability of state $i$ given observation series
$Y_1 = v_{j_1},...,Y_N = v_{j_N}$ and model parameter set $\lambda$.
For $\gamma_n(i)$, we have
\begin{align}
    \gamma_n(i) &= \frac{P \{Y_1 = v_{j_1},...,Y_N = v_{j_N}, X_n = i | \lambda \}}{P \{Y_1 = v_{j_1},...,Y_N = v_{j_N}| \lambda \}} \\
    &= \frac{P \{Y_1 = v_{j_1},...,Y_N = v_{j_N}, X_n = i | \lambda \}}{\sum_{k=1}^K P \{Y_1 = v_{j_1},...,Y_N = v_{j_N}, X_n = k | \lambda \}} \\
    &= \frac{\alpha_n(i) \beta_n(i)}{\sum_{k=1}^K \alpha_n(k) \beta_n(k)}
\end{align}
As we can see, $\gamma_n(i)$ is a probability measure satisfying $\sum_{k=1}^K \gamma_n(k) = 1$.
If
\begin{align}
    i^* = \mathop{\arg\max}_{1 \leq i \leq K} \gamma_n(i)
\end{align}
then we select $\hat{X}_n = i^*$ as the estimate of time $n$.
However, this algorithm ignore the connection between different time,
like if some $a_{ij} = 0$, some optimal series can not be reached.
\paragraph{Viterbi Algorithm}
\par
Viterbi algorithm is a progressive optimization algorithm based on \emph{Dynamic Programming},
denote
\begin{align}
    \delta_n(i) = \max_{i_1i_2...i_{n-1}} P \{ X_n = i, X_{n-1} = i_{n-1},..., X_1 = i_1; Y_n = v_{j_n},..., Y_1 = v_{j_1} |\lambda \}
\end{align}
Why $\delta_n(i)$?
Because initially we want to compute
\begin{align}
    \mathop{\arg\max}_{i_1i_2...i_{n-1}} P \{ X_n = i, X_{n-1} = i_{n-1},..., X_1 = i_1| Y_n = v_{j_n},..., Y_1 = v_{j_1}, \lambda \}
\end{align}

\end{document}